{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "import scattertext as st\n",
    "import collections\n",
    "from IPython.display import HTML, IFrame\n",
    "from textblob import TextBlob\n",
    "from w3lib.html import remove_tags\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm_notebook\n",
    "from torchtext import data\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from csv ...\n",
      "dict_keys(['review', 'label'])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split()\n",
    "\n",
    "# Step one defination of our fields. \n",
    "TEXT = data.Field(sequential=True, lower=True, tokenize=tokenizer, fix_length=100)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "print(\"loading from csv ...\")\n",
    "tv_datafields = [(\"review\", TEXT), (\"label\", LABEL)]\n",
    "\n",
    "# Step two construction our dataset.\n",
    "train, valid, test = data.TabularDataset.splits(path='',\n",
    "                                                train=\"train.csv\", validation=\"valid.csv\",\n",
    "                                                test=\"test_dataset.csv\", format=\"csv\",\n",
    "                                                skip_header=True, fields=tv_datafields)\n",
    "print(train[0].__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build vocab success...\n",
      "construct iterator success...\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, max_size = MAX_VOCAB_SIZE)\n",
    "\n",
    "print(\"build vocab success...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step four construct our iterator to our dataset. \n",
    "train_iter = data.BucketIterator(train, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n",
    "                                 sort_within_batch=False, repeat=False)\n",
    "valid_iter = data.BucketIterator(valid, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n",
    "                                 sort_within_batch=False, repeat=False)\n",
    "test_iter = data.BucketIterator(test, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n",
    "                                 sort_within_batch=False, repeat=False)\n",
    "print(\"construct iterator success...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 233929), ('and', 113202), ('a', 113038), ('of', 101442), ('to', 94437), ('is', 74279), ('in', 65120), ('it', 53784), ('i', 53168), ('this', 52561), ('that', 48504), ('br', 39942), ('was', 33534), ('as', 32463), ('for', 30754), ('with', 30625), ('movie', 29142), ('but', 29032), ('film', 26302), ('on', 23257)]\n",
      "['<unk>', '<pad>', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n"
     ]
    }
   ],
   "source": [
    "# most common words and their frequencies.\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "# top ten index to words transform.\n",
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim,bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.review).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.float())\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.review).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.float())\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.696 | Train Acc: 50.65%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 51.07%\n",
      "Epoch: 02 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.695 | Train Acc: 50.99%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 51.34%\n",
      "Epoch: 03 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.694 | Train Acc: 51.38%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.93%\n",
      "Epoch: 04 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.70%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.22%\n",
      "Epoch: 05 | Epoch Time: 0m 43s\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.90%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.72%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 39s\n",
      "\tTrain Loss: 0.692 | Train Acc: 52.18%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.41%\n",
      "Epoch: 02 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.692 | Train Acc: 52.49%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.56%\n",
      "Epoch: 03 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.44%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.49%\n",
      "Epoch: 04 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.59%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.48%\n",
      "Epoch: 05 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.79%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.38%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
