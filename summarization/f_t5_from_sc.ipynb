{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "f t5 from sc.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7btqFoi3YJx5",
        "outputId": "1693945d-b638-4dcd-f14a-070d3ae814dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install trax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/04/0c04116bbb372f459ad0a73bf306c5000f9fd63a8419bb179381f54773aa/trax-1.3.5-py2.py3-none-any.whl (416kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 4.0MB/s \n",
            "\u001b[?25hCollecting tensor2tensor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/7c/9e87d30cefad5cbc390bb7f626efb3ded9b19416b8160f1a1278da81b218/tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax) (0.3.0)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax) (2.1.0)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/a4/1188812f1e439bba23f7ceb03879bcdb99744e2e12799aebb93bc0139496/t5-0.7.0-py3-none-any.whl (171kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax) (0.17.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax) (1.4.1)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax) (0.1.55)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (1.7.12)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 33.8MB/s \n",
            "\u001b[?25hCollecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 34.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (2.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (4.1.2.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (2.23.0)\n",
            "Collecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 28.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (4.1.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (1.1.2)\n",
            "Collecting mesh-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8b/553deb763ce8d00afb17debab7cb14a87b209cd4c6f0e8ecfc8d884cb12a/mesh_tensorflow-0.1.17-py3-none-any.whl (342kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (1.1.1)\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (1.0.5)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (0.8.3)\n",
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 35.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (0.16.0)\n",
            "Collecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.5MB/s \n",
            "\u001b[?25hCollecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/36/06fe2c757044bb51906fef231ac48cc5bf9a277fc9a8c7e1108d7e9e8cfd/kfac-0.2.3-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 40.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (4.41.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.12.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (20.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.3.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.24.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.6.0+cu101)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax) (2.8.0)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.1.2)\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.7MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/75/679a8312490388eb6cb70c8729020a3d7e721e0aefabf2d75a7cbefdb82a/tfds_nightly-4.0.1.dev202010160106-py3-none-any.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 29.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax) (3.0.1)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/89/1eb9dbb9e24f5e2c29ab1a88097b2f1333858aac3cd3cccc6c4c1c8ad867/zope.interface-5.1.2-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 43.4MB/s \n",
            "\u001b[?25hCollecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent->tensor2tensor->trax) (50.3.0)\n",
            "Collecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax) (2.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->trax) (4.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax) (0.2.8)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax) (2.11.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->tensor2tensor->trax) (1.1.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tensor2tensor->trax) (2.7.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gan->tensor2tensor->trax) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5->trax) (2018.9)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Collecting importlib-resources; python_version < \"3.9\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5->trax) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.35.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->tensor2tensor->trax) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor->trax) (1.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->t5->trax) (3.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.0)\n",
            "Building wheels for collected packages: pypng, bz2file, sacremoses\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp36-none-any.whl size=67162 sha256=22e494987352f42fbe63f2ec144ba01401cceb3d3515b2acb1aa5db9453f89a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp36-none-any.whl size=6884 sha256=cd834ddadde354dd1db3b3e18d98e419d95b79ca7f7ecde48bc676e7576df855\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=40d4b265a7407cb660aba24ec10b2b5cefac8afe76b8f68781d3220ca44823c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built pypng bz2file sacremoses\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tf-slim, zope.interface, greenlet, zope.event, gevent, tensorflow-probability, mesh-tensorflow, pypng, bz2file, gunicorn, kfac, tensorflow-gan, tensor2tensor, portalocker, sacrebleu, sacremoses, tokenizers, sentencepiece, transformers, rouge-score, importlib-resources, tfds-nightly, tensorflow-text, t5, funcsigs, trax\n",
            "  Found existing installation: tensorflow-probability 0.11.0\n",
            "    Uninstalling tensorflow-probability-0.11.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.11.0\n",
            "Successfully installed bz2file-0.98 funcsigs-1.0.2 gevent-20.9.0 greenlet-0.4.17 gunicorn-20.0.4 importlib-resources-3.0.0 kfac-0.2.3 mesh-tensorflow-0.1.17 portalocker-2.0.0 pypng-0.0.20 rouge-score-0.0.4 sacrebleu-1.4.14 sacremoses-0.0.43 sentencepiece-0.1.91 t5-0.7.0 tensor2tensor-1.15.7 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tensorflow-text-2.3.0 tf-slim-1.1.0 tfds-nightly-4.0.1.dev202010160106 tokenizers-0.8.1rc2 transformers-3.3.1 trax-1.3.5 zope.event-4.5.0 zope.interface-5.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1h8dHsaWaym"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as jnp\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhK--Z9wWyam"
      },
      "source": [
        "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
        "                                 keys=('article', 'highlights'),\n",
        "                                 train=True)\n",
        "\n",
        "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
        "                                keys=('article', 'highlights'),\n",
        "                                train=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhEOlxeW6bS"
      },
      "source": [
        "def tokenize(input_str, EOS=1):\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      # vocab_dir='vocab_dir/',\n",
        "                                      vocab_file='en_8k.subword'))\n",
        "    \n",
        "    return list(inputs) + [EOS]\n",
        "\n",
        "def detokenize(integers):  \n",
        "    s = trax.data.detokenize(integers,\n",
        "                            #  vocab_dir='vocab_dir/',\n",
        "                             vocab_file='en_8k.subword')\n",
        "    \n",
        "    return wrapper.fill(s)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8qmUFB0YogO"
      },
      "source": [
        "# Special tokens\n",
        "SEP = 0 # Padding or separator token\n",
        "EOS = 1 # End of sentence token\n",
        "\n",
        "# Concatenate tokenized inputs and targets using 0 as separator.\n",
        "def preprocess(stream):\n",
        "    for (article, summary) in stream:\n",
        "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
        "        yield joint, joint, np.array(mask)\n",
        "\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\n",
        "input_pipeline = trax.data.Serial(\n",
        "    # Tokenizes\n",
        "    trax.data.Tokenize(#vocab_dir='vocab_dir/',\n",
        "                       vocab_file='en_8k.subword'),\n",
        "    # Uses function defined above\n",
        "    preprocess,\n",
        "    # Filters out examples longer than 2048\n",
        "    trax.data.FilterByLength(2048)\n",
        ")\n",
        "\n",
        "# Apply preprocessing to data streams.\n",
        "train_stream = input_pipeline(train_stream_fn())\n",
        "eval_stream = input_pipeline(eval_stream_fn())\n",
        "\n",
        "train_input, train_target, train_mask = next(train_stream)\n",
        "\n",
        "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj9oFdMzeVe2",
        "outputId": "0f476403-4216-48ae-eb0d-d270291ab55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        }
      },
      "source": [
        "print(f'Single example mask:\\n\\n {train_mask}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single example mask:\n",
            "\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_undGODftGh",
        "outputId": "86fa2fbb-28da-40c0-b3d6-6349c2fedacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f'Single example:\\n\\n {detokenize(train_input)}')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single example:\n",
            "\n",
            " A new study is backing up long held suspicions that Apple slows down\n",
            "older models of its iPhones to encourage users to buy a new release.\n",
            "The U.S. study analysed worldwide searches for 'iPhone slow' and found\n",
            "that the search term spiked significantly around the time of new\n",
            "iPhone launch. It then compared those results with similar searches\n",
            "for the term 'Samsung Galaxy slow', and discovered the term was\n",
            "unaffected by new releases from Samsung. A new study is backing up\n",
            "long held suspicions that Apple slows down older models of iPhones to\n",
            "encourage users to buy its new release. The U.S. study analysed\n",
            "worldwide searches for 'iPhone slow' and found that the search term\n",
            "spiked significantly around the time of new phone releases . The\n",
            "study, compiled by Harvard University PhD student Laura Trucco,\n",
            "follows claims that the Cupertino-based company is deliberately\n",
            "sabotaging its old products. Writing for the New York Times, Sendhil\n",
            "Mullainathan, a professor of economics at Harvard, described the\n",
            "results as 'striking'. 'Wouldn't many business owners love to make\n",
            "their old product less useful whenever they released a newer one?' Mr\n",
            "Mullainathan wrote. ‘When you sell the device and control the\n",
            "operating system, that's an option'. The study then compared those\n",
            "results with similar searches for the term 'Samsung Galaxy slow', and\n",
            "found the term was unaffected by new releases from Samsung . While\n",
            "some MailOnline readers haven't noticed a slow down, others claim that\n",
            "Apple is sabotaging older phones through software updates. 'This is\n",
            "common knowledge,' one reader wrote. 'If you want to keep your iPhone\n",
            "running at the same pace do not do the software upgrade that comes out\n",
            "within six months of a new iPhone release,' Last year, Catherine\n",
            "Rampell, also writing in the New York Times, raised concerns that\n",
            "Apple could be engineering the new operating system so it only works\n",
            "properly with the newest version of the product. She said her iPhone 4\n",
            "became a lot slower when she downloaded iOS 7 - and that the only\n",
            "solution seemed to be to buy the iPhone 5. Ms Rampell accused Apple of\n",
            "having run out of ideas so was trying to ‘brainwash’ its customers\n",
            "into buying the new iPhone 5S and 5C because they look nice. Her\n",
            "claims fuelled conspiracy theorists who have long held that Apple\n",
            "engages in ‘planned obsolescence’, a term which has been around since\n",
            "the Great Depression in the 1930s. The theory states that\n",
            "manufacturers of everything from cars to microwaves build in a certain\n",
            "lifetime to a product and then it will simply stop working, forcing\n",
            "consumers to buy a new one. And Apple has faced allegations that it is\n",
            "guilty of planned obsolescence before. Last year, Catherine Rampell,\n",
            "also writing in the New York Times, raised concerns that Apple could\n",
            "be engineering the new operating system so it only works properly with\n",
            "the newest version of the product . A security expert has warned\n",
            "Apple’s iOS software contains potentially sinister tools that could be\n",
            "used by governments to spy on iPhone and iPad users. Speaking at the\n",
            "'Hackers on planet Earth' conference in New York, Jonathan Zdziarski\n",
            "said that most users are unaware of the lack of protection for iPhone\n",
            "data. He added files found hidden within the firm's software contain a\n",
            "file-relay service that can be used to access the user's address book,\n",
            "photos, voicemail and any accounts configured on the device. However,\n",
            "Apple has denied the claims the backdoor was created deliberately for\n",
            "government or surveillance purposes. His investigation followed\n",
            "earlier reports of the NSA spying on Apple products, which suggested a\n",
            "‘backdoor’ in iOS could provide hackers with valuable information. A\n",
            "backdoor is a hidden remote access port that can allow outside sources\n",
            "to access a device with little detection. The conclusion was based on\n",
            "an analysis of 600 million iOS devices, with handsets running the most\n",
            "recent versions of the software at particular risk. When it started\n",
            "using more tamper-resistant screws experts said it was to stop users\n",
            "getting into the phone and fixing it themselves if there was a\n",
            "problem. Meanwhile, in 2012 Apple was sued in Brazil by the Brazilian\n",
            "Institute of Politics and Law Software over the launch of the iPad\n",
            "Air. The organisation claimed that because it had the new retina\n",
            "screen it made the iPad 3 redundant and that Apple was changing its\n",
            "devices too quickly. Ms Rampell said: ‘When major innovations remain\n",
            "out of reach, and degrading durability threatens to tick off loyal\n",
            "customers, companies like Apple can still take a cue from the fashion\n",
            "industry. ‘If you can brainwash consumers into developing new tastes\n",
            "that make the old stuff look uncool for aesthetic rather than\n",
            "functional reasons, you still have a shot at harvesting more sales\n",
            "from your existing customer base. Dom Ferkin, managing direction of\n",
            "UK-based iOS experts, Creation Application, told MailOnline that he\n",
            "doesn’t believe Apple are doing this intentionally. ‘On every hardware\n",
            "release they tend to upgrade the chips and they are faster every time\n",
            "they are released,’ he said. ‘Each year they release a new iOS. If\n",
            "you’re running an iOS 7 on a 5 chip, for example, it’s comparable to\n",
            "running Windows XP on a Windows 95 machine. ‘It’s just enough to annoy\n",
            "the users, but it’s needed if you want the slew of new features that\n",
            "Apple releases each year.’ Mr Mullainathan added that the research\n",
            "does not prove that Apple has done anything wrong. No matter how\n",
            "suggestive, he says, the data alone doesn't allow anyone to determine\n",
            "conclusively whether their phone is any slower. There are other\n",
            "explanations for why an older model iPhone may slow down, he claims.\n",
            "For instance, the latest version of the Apple operating system, iOS,\n",
            "is always tailored to the newest device and may therefore not work as\n",
            "efficiently on older models. ‘Hearing about a new release makes you\n",
            "contemplate getting a new and faster phone,’ he added. ‘And you\n",
            "suddenly notice how slow your old phone is.’ Apple is yet to respond.\n",
            "But Mr Mullainathan added that the research does not prove that Apple\n",
            "has done anything wrong.No matter how suggestive, he says, the data\n",
            "alone doesn't allow anyone to determine conclusively whether their\n",
            "phone is any slower .<EOS><pad>Thestudy was undertaken by student\n",
            "Laura Trucco at Harvard University . It also compared Apple's results\n",
            "with searches for 'Samsung Galaxy slow' Research found that the term\n",
            "was unaffected by Samsung new releases . Study has fuelled suggestions\n",
            "Apple engages in ‘planned obsolescence’ Theory states that\n",
            "manufacturers build in a certain lifetime to a product and then it\n",
            "will simply stop working, forcing consumers to buy a new one .<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Lf-9d1fvwS"
      },
      "source": [
        "\n",
        "# Bucketing to create batched generators.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256,\n",
        "# 4 of length < 512. And so on. \n",
        "boundaries =  [128, 256,  512, 1024]\n",
        "batch_sizes = [16,    8,    4,    2, 1]\n",
        "\n",
        "# Create the streams.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes)(train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-_O4AzTgCwq",
        "outputId": "23312d51-d612-4dbb-805a-f258a561b24e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "# Every execution will result in generation of a different article\n",
        "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
        "input_batch, _, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# Shape of the input_batch\n",
        "input_batch.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1111)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR9qeNofgE7j",
        "outputId": "448f52a0-2bff-4e6d-a506-2e37774b06f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(input_batch[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1411 4030   84 4173  900   47  492 5182   57 4030   84  205  262  212\n",
            "  289 3849  297  137  274 3536    8  463  113 5396 3319  385    3  479\n",
            "  636  388 4030 4352 4030   84  262  205 6355  284  137  274 3536    8\n",
            "  619  113 6185 3319  385    3  479  636  388 4030   84   87 4873  513\n",
            " 1187  140 4127   23   13 3528 3968 4651   91 5966    6 1546   22   39\n",
            "    4 4297    2    5 3304   20 5689  104 1225  118    5    4 2035 3000\n",
            "  138 2840 5124    2 3202  409   10 2589    8 1237   29 3389  820    3\n",
            "  824    3   51 6596 4579 2438    3  835 2785   39    4 6044  232  861\n",
            "   31 1476  619  255   12  282 5273 2152 6309  229    8 3111   36   20\n",
            " 5044   20   35   71  172 2440   32  245 4647 1854   36    6 1342   30\n",
            "   12 4689    3   43 5273   37  310  245 4127    2   13 1176    4 5273\n",
            "  626  484  828 4040   35 5966    6    8 1237   29 3389  820    3  824\n",
            " 3779 7021   84  103  245   82 6223   47   58   12 2840 5124    2 3202\n",
            "  409  255   12  282 5273 2152 6309  229 3614   20  619 4030   84 3935\n",
            "    6 3389  820 5720  245 6236    2   80  595 3329   23   34  245 4880\n",
            "    5 4732  121  861  349 5491    6    4  282 4030   84 3935    6 3389\n",
            "  820    3 2258  592  481 5242    2    7  656 1241 5886 3663 2048 2547\n",
            "   47 1607 1839    2 5952 6342 2299   29    3 1034    3   25  309 2408\n",
            " 5437   20   34  245 2499   78    3 5720    4   82 6044 1342  861 2519\n",
            "   31  245 4647 1854   36    6  232  861   31  235    8 3111 1125 3536\n",
            "   36 5898    2 4438   13  495  115  375   51 1685    3  121  139   31\n",
            " 5854   23   13   71 1254  139 2595   13  897  245   10    4  872 2796\n",
            "    2    5 2368   47 1202   36  139   31 5632 5293   20   13  643  387\n",
            "  245   26    4 4030   84 2626 1986    7 6325  680 4476 3268   22  121\n",
            " 5801   67  861 4992   42 1887   47 3286   32  245 2600 1202   36  139\n",
            " 5012   23 4030   84 2324   29  266   80  629  911  333 4078   91  292\n",
            "    5  245 4806    2    7    4 3379   67   31   37 4030   84  861 1887\n",
            "   20 1476 2639  292    5    4 5518 2699    8  139   31 1476   12 4689\n",
            " 1254  121    4  520 4030   84  104  307 3578   39  562  961  244 1343\n",
            "  246 3935    6 3389  820 5720  861 4972    6   37    4 5974 4763    2\n",
            " 1197 4030   84 4078   91   51  245 4647 1854 1887    6 4806    2  449\n",
            "   96  133 3284   23   34 4630 2707   32 4030   84    4 1342   10   12\n",
            " 2752  379    8 3111 1125 3536   36 3111  449   96 3286   23 4030   84\n",
            " 4188    8  179 1887    6 1081   13 6107   71  257 1254  523   37 1887\n",
            "    6    4  164  173 4030   84 2090 1887  818  897 1824  118    8  139\n",
            " 4127    2  287   13 1005 3154   39 2730 1742   32 4030   84 2061 4275\n",
            "    6  961  244 3935    6 3389  820  104  133 2785   35 2840 5124    2\n",
            " 3202  409 1084   57 2232   22    7 3278 5374   10 6596 4579 2438   35\n",
            "  238   12  130 4030   84 3935    6 3389  820    7  245  592  387    3\n",
            " 2840 5124    2 1336  559 1047 2010 3986 2613   64  882  478 2699    3\n",
            "   10 4896    5  118    5  124 4687 1611 4030   84 3935    6 3389  820\n",
            "   36    6  592  481 5242    2    7  656 1241 5886 3663 2048 2547   47\n",
            " 1607 1839    2 5952 6342 2299   29    3 1034    3   25 3600   22  141\n",
            "   13  245  309 5456   49 2499   78 4030  916 1158    4  322    5    4\n",
            "  288    3 1156 1887  211 3906   32   39 1824  118 1887    6 3304   20\n",
            "  118    8  179 1887    6   90 1476   12 1342  961  244   87  130 2663\n",
            "    2    3 3935    6 3389  820  835 2785 1642   14  173   58 1084   57\n",
            " 2232   22    7 3278 5374 2840 5124    2 3202  409   10 6596 4579 2438\n",
            "    8 1210    5    4  759   36    6 2035 3000  138 1000 4824  167    3\n",
            "  861 1125  245 4732 4880  595 3329   23  194 1202   36 1574    2 6236\n",
            "    2   80  595 3329   23    7 5012   23  151 6420    6    5 6499   67\n",
            " 4013    6 6255   84  861 1125 1202   36  755 5012   23  562  442    4\n",
            " 6513   44 6366    2  280 4030   84  232  162   80  339 1690  757   23\n",
            "  961 7021   84  139 3254   20  335   37   31 1476    4 4030   84 2922\n",
            " 4078   91  292    5    4 4724    6  961  244 3951  513 1082   10    4\n",
            "  933 2931   42   61 3935    6 3389  820   25 5351 1143   35   12 5324\n",
            "   10 2840 5124    2 3209   30  258   30 2785 1642   14  173 4030   84\n",
            " 1166  861   25   90 4252   32 2840 5124    6    3 3935    6 3389  820\n",
            " 3393    6 5822   22 4054    2    7 1054  193   13  245 1854 3187  852\n",
            " 6244    3 4678 4030   84  179 2644 1240   23  245   13 1782  280   12\n",
            " 1642   14  173 5504   58 1084   57 2232   22    7 3278 5374 2840 5124\n",
            "    2 3202  409   10 6596 4579 2438   12  130 2663    2 1254 6277  118\n",
            "    5    4 2035 3000  138 1000 4824  167    8 1343  246  861   25 4253\n",
            "   32   12 5141 4902    2   58    4 1336  559 1047    3 3403 3275 3461\n",
            "   12 2840 5124    2 3209 3445  123 5324    7   43 4630 2707   32 4902\n",
            "    2   58    4  269    5 3707   97    8 3313   20   10  856 4020    2\n",
            "    3 1908 5621    2    7  791 1082 1919  193    3 3935    6 3389  820\n",
            " 4252   23  245   82 2840 5124    2   10  789  164  130    7  104 5248\n",
            " 1066 4423    2  528  350    8   28 6596 4579 2438 4873  513 1187   10\n",
            " 1642 2840 5124    2  958   78    3  361    3    7   39 2929  406  363\n",
            "   26 2195 3393   29   35   12 6335    2  292    3  257 4030   84 1105\n",
            " 3044   32 4687 1611   25  151   10   12  288   36    6  282   35  824\n",
            "   14  130   14 1249 1237   29    3  140  538 3669   13 4072  287   36\n",
            "    6 3304   20 5689   12  465 4590 3003 1159   20 1016 4030    2    1\n",
            "    0 1237   29 3389  820    3  824    3   51 6596 4579 2438    3  104\n",
            " 4871   10   12 2840 5124    2 1336  559 1047   35   12  130 4030  210\n",
            " 1105 3044   32 4687 1611    3  513 1206   91  933 2931  220    7 4252\n",
            "   32 3209  372    5  245 2499   78 4030  210 3111 5720 3536   36  179\n",
            "   36    6    4  164  173 2090  897  335 1254   71   36    6 1081   13\n",
            " 6107   71  257   36    1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD5LSMwWgIzu",
        "outputId": "dc337813-6074-4950-85f9-01dc061af80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Article:\\n\\n', detokenize(input_batch[0]))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article:\n",
            "\n",
            " By . Harriet Arkell . PUBLISHED: . 11:56 EST, 15 May 2013 . | .\n",
            "UPDATED: . 14:54 EST, 15 May 2013 . A teenager who wanted to help\n",
            "grieving relatives cope with the loss of loved ones has become one of\n",
            "the youngest funeral directors in Britain. Amy Darlow, 19, from\n",
            "Salisbury, began working with the dead when she was just 14 during a\n",
            "work experience placement. She'd applied for it after seeing her\n",
            "grandmother's body as a child, an experience that made her want to\n",
            "make the experience less traumatic for relatives. Amy Darlow, 19,  had\n",
            "her first stint at a funeral directors during a work experience\n",
            "placement aged 14 . Miss Darlow says her friends were surprised by her\n",
            "choice of career but she enjoys the work . Miss Darlow, whose\n",
            "boyfriend and childhood sweetheart Declan Spreadbury, 22, is\n",
            "untroubled by her job, says the first dead body she saw was her\n",
            "grandmother's when she was 10. She said: 'People try to hide death\n",
            "from children, but I was exposed to it - I went to see her in the\n",
            "Chapel of Rest. 'I was encouraged to kiss her on the . forehead and\n",
            "say goodbye but sadly she wasn’t looking her best. 'I asked . why\n",
            "there were bubbles coming out of her mouth and the reply was that .\n",
            "she’d just come out of the fridge. I was just a child - but the image\n",
            ". has stayed with me.' Now Miss Darlow says she knows that the gastric\n",
            "air . coming from her grandmother’s mouth could have been prevented by\n",
            "embalming . the body in a certain way. She said: 'She could have\n",
            "looked . better. It’s important to get it right - because that’s the\n",
            "last time . you’ll see someone. I want people to go away with good\n",
            "lasting . memories.' Miss Darlow has been working for funeral\n",
            "directors Will Case and Partners in Salisbury for over a year . Miss\n",
            "Darlow and her boss, funeral parlour manager Christopher Orledge, in\n",
            "front of one of their hearses . Miss Darlow's boyfriend and childhood\n",
            "sweetheart Declan Spreadbury, 22, is quite used to her unusual job .\n",
            "'At the end of the day, we’re dealing with someone’s loved one. It’s\n",
            "not just a body.' A year ago, Miss Darlow began working full-time at\n",
            "Will Case and Partners funeral directors in Salisbury. One of the\n",
            "country's youngest undertakers, she said her career choice surprised\n",
            "some. 'My friends were surprised and asked all sorts of silly\n",
            "questions,' she said. 'They asked me if the bodies sat up . when they\n",
            "were being cremated.  I told them that was just the . gas coming out\n",
            "of the organs.' Another nail in the coffin: Miss Darlow is studying\n",
            "for a degree in funeral services as well as working full-time . When\n",
            "she is not conducting funerals, Miss Darlow reads crime fiction and\n",
            "chats to her mother Elizabeth, 53 . It prompted her to take up a full-\n",
            "time post at Will Case and Partners funeral directors in Salisbury a\n",
            "year ago - becoming one of the youngest undertakers. Now she is\n",
            "completing a training course at the parlour, alongside a funeral\n",
            "services foundation degree and an embalming course at the University\n",
            "of Bath. Clad in top hat, cravat and tail coats, Miss Darlow conducted\n",
            "her first funeral in June last year and has completed another nine\n",
            "since then. The Salisbury teenager in full funeral garb, left, and\n",
            "with tattoos on show ready for a night out, right . Polishing hearses\n",
            "is all in a day's work for 19-year-old Amy, who likes to give people's\n",
            "loved ones a dignified send off .<EOS><pad>AmyDarlow, 19, from\n",
            "Salisbury, has worked in a funeral parlour for a year . Polishing\n",
            "hearses, nailing coffins and conducting services part of her job . She\n",
            "says: 'It's the last time you see them - it's important to get it\n",
            "right'<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZCWbx6rgTNt"
      },
      "source": [
        "def create_tensor(t):\n",
        "    \"\"\"Create tensor from list of lists\"\"\"\n",
        "    return jnp.array(t)\n",
        "\n",
        "\n",
        "def display_tensor(t, name):\n",
        "    \"\"\"Display shape and tensor\"\"\"\n",
        "    print(f'{name} shape: {t.shape}\\n')\n",
        "    print(f'{t}\\n')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9GD26IAgh-d",
        "outputId": "3d58a269-9f8f-4e4a-9639-e291d89be5b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
        "display_tensor(q, 'query')\n",
        "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
        "display_tensor(k, 'key')\n",
        "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
        "display_tensor(v, 'value')\n",
        "m = create_tensor([[0, 0], [-1e9, 0]])\n",
        "display_tensor(m, 'mask')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query shape: (2, 3)\n",
            "\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "key shape: (2, 3)\n",
            "\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "value shape: (2, 3)\n",
            "\n",
            "[[0 1 0]\n",
            " [1 0 1]]\n",
            "\n",
            "mask shape: (2, 2)\n",
            "\n",
            "[[ 0.e+00  0.e+00]\n",
            " [-1.e+09  0.e+00]]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
            "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siyQa5nIgkfb",
        "outputId": "69df9838-f7f5-46ed-e867-1d5eead5e39e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "\n",
        "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
        "display_tensor(q_dot_k, 'query dot key')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query dot key shape: (2, 2)\n",
            "\n",
            "[[0.57735026 2.309401  ]\n",
            " [1.1547005  2.8867514 ]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4xdaXXGgp28",
        "outputId": "ebc07340-d522-4eec-ad3a-28f21457253c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "masked = q_dot_k + m\n",
        "display_tensor(masked, 'masked query dot key')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "masked query dot key shape: (2, 2)\n",
            "\n",
            "[[ 5.7735026e-01  2.3094010e+00]\n",
            " [-1.0000000e+09  2.8867514e+00]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_DSyr5Ogt1H",
        "outputId": "131ecc80-dbe3-42ba-fdf0-7d593d701b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "display_tensor(masked @ v, 'masked query dot key dot value')\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "masked query dot key dot value shape: (2, 3)\n",
            "\n",
            "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
            " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpAO0KJugxqu",
        "outputId": "c916ffbd-a555-4c88-86c7-6d67be2f50bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "q_with_batch = q[None,:]\n",
        "display_tensor(q_with_batch, 'query with batch dim')\n",
        "k_with_batch = k[None,:]\n",
        "display_tensor(k_with_batch, 'key with batch dim')\n",
        "v_with_batch = v[None,:]\n",
        "display_tensor(v_with_batch, 'value with batch dim')\n",
        "m_bool = create_tensor([[True, True], [False, True]])\n",
        "display_tensor(m_bool, 'boolean mask')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n",
            "key with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[1 2 3]\n",
            "  [4 5 6]]]\n",
            "\n",
            "value with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[0 1 0]\n",
            "  [1 0 1]]]\n",
            "\n",
            "boolean mask shape: (2, 2)\n",
            "\n",
            "[[ True  True]\n",
            " [False  True]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNRl0Oo3g0wA"
      },
      "source": [
        "def DotProductAttention(query, key, value, mask):\n",
        "    \"\"\"Dot product self-attention.\n",
        "    Args:\n",
        "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
        "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
        "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
        "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
        "\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
        "    \"\"\"\n",
        "\n",
        "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
        "    depth = query.shape[-1] \n",
        "\n",
        "    # Calculate scaled query key dot product according to formula above\n",
        "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
        "\n",
        "    # Apply the mask\n",
        "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
        "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
        "    \n",
        "    # Softmax formula implementation\n",
        "    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
        "    # Hint: Last axis should be used and keepdims should be True\n",
        "    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
        "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
        "\n",
        "    # Take exponential of dots minus logsumexp to get softmax\n",
        "    # Use jnp.exp()\n",
        "    dots = jnp.exp(dots - logsumexp)\n",
        "\n",
        "    # Multiply dots by value to get self-attention\n",
        "    # Use jnp.matmul()\n",
        "    attention = jnp.matmul(dots, value)\n",
        "\n",
        "    ## END CODE HERE ###\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2ZXh5nCg5YJ",
        "outputId": "15aa905d-f098-4533-d4e9-5f0e8e60f431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
              "              [1.        , 0.        , 1.        ]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkzhjRlEg7LB",
        "outputId": "074d7606-4452-4f68-82b0-72158f0f689e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "tensor2d = create_tensor(q)\n",
        "display_tensor(tensor2d, 'query matrix (2D tensor)')\n",
        "\n",
        "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
        "display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')\n",
        "\n",
        "tensor3dc = create_tensor([jnp.concatenate([q, q], axis = -1)])\n",
        "display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')\n",
        "\n",
        "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
        "display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query matrix (2D tensor) shape: (2, 3)\n",
            "\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n",
            "\n",
            "[[[[1 0 0]\n",
            "   [0 1 0]]\n",
            "\n",
            "  [[1 0 0]\n",
            "   [0 1 0]]]\n",
            "\n",
            "\n",
            " [[[1 0 0]\n",
            "   [0 1 0]]\n",
            "\n",
            "  [[1 0 0]\n",
            "   [0 1 0]]]]\n",
            "\n",
            "one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n",
            "three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZKCAkozg9_R"
      },
      "source": [
        "\n",
        "def compute_attention_heads_closure(n_heads, d_head):\n",
        "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
        "    Args:\n",
        "        d_head (int):  dimensionality of heads.\n",
        "        n_heads (int): number of attention heads.\n",
        "    Returns:\n",
        "        function: compute_attention_heads function\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_attention_heads(x):\n",
        "        \"\"\" Compute the attention heads.\n",
        "        Args:\n",
        "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "        Returns:\n",
        "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "        \"\"\"\n",
        "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "        \n",
        "        # Size of the x's batch dimension\n",
        "        batch_size = x.shape[0]\n",
        "        # Length of the sequence\n",
        "        # Should be size of x's first dimension without counting the batch dim\n",
        "        seqlen = x.shape[1]\n",
        "        \n",
        "        \n",
        "        # Reshape x using jnp.reshape()\n",
        "        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
        "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
        "        \n",
        "        \n",
        "        # Transpose x using jnp.transpose()\n",
        "        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
        "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
        "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "        \n",
        "        \n",
        "        # Reshape x using jnp.reshape()\n",
        "        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
        "        x = jnp.reshape(x, (-1, seqlen, d_head))\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    return compute_attention_heads"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiFyZon2hEy2",
        "outputId": "9ca1c645-d796-4a1a-e40f-f972420be704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "\n",
        "display_tensor(tensor3dc3b, \"input tensor\")\n",
        "result_cah = compute_attention_heads_closure(2,3)(tensor3dc3b)\n",
        "display_tensor(result_cah, \"output tensor\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input tensor shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n",
            "output tensor shape: (6, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXhxceHXhG5_"
      },
      "source": [
        "def dot_product_self_attention(q, k, v):\n",
        "    \"\"\" Masked dot product self attention.\n",
        "    Args:\n",
        "        q (jax.interpreters.xla.DeviceArray): queries.\n",
        "        k (jax.interpreters.xla.DeviceArray): keys.\n",
        "        v (jax.interpreters.xla.DeviceArray): values.\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
        "    # NOTE: there is a revision underway with the autograder to tolerate better indexing. \n",
        "    # Until then, please index q.shape using negative values (this is equivalent to counting from right to left)\n",
        "    mask_size = q.shape[-2]\n",
        "\n",
        "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
        "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
        "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
        "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return DotProductAttention(q, k, v, mask)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGxPp0GBhLPM",
        "outputId": "d8ba9ac1-f1f2-4181-8657-8a8acc81225e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[0.        , 1.        , 0.        ],\n",
              "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORMaBeD1hWSy"
      },
      "source": [
        "def compute_attention_output_closure(n_heads, d_head):\n",
        "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
        "    Args:\n",
        "        d_head (int):  dimensionality of heads.\n",
        "        n_heads (int): number of attention heads.\n",
        "    Returns:\n",
        "        function: compute_attention_output function\n",
        "    \"\"\"\n",
        "    \n",
        "    def compute_attention_output(x):\n",
        "        \"\"\" Compute the attention output.\n",
        "        Args:\n",
        "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "        Returns:\n",
        "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "        \"\"\"\n",
        "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "        \n",
        "        # Length of the sequence\n",
        "        # Should be size of x's first dimension without counting the batch dim\n",
        "        seqlen = x.shape[1]\n",
        "        \n",
        "        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
        "        x = jnp.reshape(x, ( -1, n_heads, seqlen, d_head))\n",
        "\n",
        "        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
        "        x = jnp.transpose(x, ( 0, 2, 1 , 3))\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Reshape to allow to concatenate the heads\n",
        "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
        "    \n",
        "    return compute_attention_output"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqWNDYpfha1h",
        "outputId": "883a07ea-10f1-4275-856d-d83047869999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "display_tensor(result_cah, \"input tensor\")\n",
        "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
        "display_tensor(result_cao, \"output tensor\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input tensor shape: (6, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n",
            "output tensor shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8hob3LRhdDF"
      },
      "source": [
        "\n",
        "def CausalAttention(d_feature, \n",
        "                    n_heads, \n",
        "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
        "                    dot_product_self_attention=dot_product_self_attention,\n",
        "                    compute_attention_output_closure=compute_attention_output_closure,\n",
        "                    mode='train'):\n",
        "    \"\"\"Transformer-style multi-headed causal attention.\n",
        "\n",
        "    Args:\n",
        "        d_feature (int):  dimensionality of feature embedding.\n",
        "        n_heads (int): number of attention heads.\n",
        "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
        "        dot_product_self_attention (function): dot_product_self_attention function. \n",
        "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
        "        mode (str): 'train' or 'eval'.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
        "    \"\"\"\n",
        "    \n",
        "    assert d_feature % n_heads == 0\n",
        "    d_head = d_feature // n_heads\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
        "    # Since you are dealing with closures you might need to call the outer \n",
        "    # function with the correct parameters to get the actual uncalled function.\n",
        "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
        "    \n",
        "    \n",
        "    return tl.Serial(\n",
        "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
        "        ),\n",
        "        \n",
        "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
        "        # HINT: The second argument to tl.Fn() is an uncalled function\n",
        "        # Since you are dealing with closures you might need to call the outer \n",
        "        # function with the correct parameters to get the actual uncalled function.\n",
        "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
        "        tl.Dense(d_feature) # Final dense layer\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJVFUf-Lhkd2",
        "outputId": "0323883d-3ba8-44f1-f288-c94648d67ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "print(CausalAttention(d_feature=512, n_heads=8))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Branch_out3[\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "  ]\n",
            "  DotProductAttn_in3\n",
            "  AttnOutput\n",
            "  Dense_512\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtGgcV8xhm-A"
      },
      "source": [
        "\n",
        "def DecoderBlock(d_model, d_ff, n_heads,\n",
        "                 dropout, mode, ff_activation):\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
        "\n",
        "    The input is an activation tensor.\n",
        "\n",
        "    Args:\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        mode (str): 'train' or 'eval'.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # Create masked multi-head attention block using CausalAttention function\n",
        "    causal_attention = CausalAttention( \n",
        "                        d_model,\n",
        "                        n_heads=n_heads,\n",
        "                        mode=mode\n",
        "                        )\n",
        "\n",
        "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
        "    feed_forward = [ \n",
        "        # Normalize layer inputs\n",
        "        tl.LayerNorm(),\n",
        "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
        "        tl.Dense(d_ff),\n",
        "        # Add activation function passed in as a parameter (you need to call it!)\n",
        "        ff_activation(), # Generally ReLU\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
        "        tl.Dense(d_model),\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "        tl.Dropout(rate=dropout,mode=mode)\n",
        "    ]\n",
        "\n",
        "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
        "    return [\n",
        "      tl.Residual(\n",
        "          # Normalize layer input\n",
        "          tl.LayerNorm(),\n",
        "          # Add causal attention block previously defined (without parentheses)\n",
        "          causal_attention,\n",
        "          # Add dropout with rate and mode specified\n",
        "          tl.Dropout(rate=dropout, mode=mode)\n",
        "        ),\n",
        "      tl.Residual(\n",
        "          # Add feed forward block (without parentheses)\n",
        "          feed_forward\n",
        "        ),\n",
        "      ]\n",
        "    ### END CODE HERE ###"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcZIP0vjhtMV",
        "outputId": "1b933e31-add6-4887-cdd1-0bcf43f121eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Serial[\n",
            "        Branch_out3[\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "        ]\n",
            "        DotProductAttn_in3\n",
            "        AttnOutput\n",
            "        Dense_512\n",
            "      ]\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "], Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Dense_2048\n",
            "      Relu\n",
            "      Dropout\n",
            "      Dense_512\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTYrt0WbhvzN"
      },
      "source": [
        "\n",
        "def TransformerLM(vocab_size=33300,\n",
        "                  d_model=512,\n",
        "                  d_ff=2048,\n",
        "                  n_layers=6,\n",
        "                  n_heads=8,\n",
        "                  dropout=0.1,\n",
        "                  max_len=4096,\n",
        "                  mode='train',\n",
        "                  ff_activation=tl.Relu):\n",
        "    \"\"\"Returns a Transformer language model.\n",
        "\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\n",
        "    decoder part of the overall Transformer.)\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): vocab size.\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_layers (int): number of decoder layers.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        max_len (int): maximum symbol length for positional encoding.\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
        "        to activations over a vocab set.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # Embedding inputs and positional encoder\n",
        "    positional_encoder = [ \n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\n",
        "        tl.Embedding(vocab_size, d_model),\n",
        "        # Use dropout with rate and mode specified\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        # Add positional encoding layer with maximum input length and mode specified\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
        "\n",
        "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
        "    decoder_blocks = [ \n",
        "        DecoderBlock(d_model, d_ff, n_heads,\n",
        "                    dropout, mode, ff_activation) for _ in range(n_layers)]\n",
        "\n",
        "    # Create the complete model as written in the figure\n",
        "    return tl.Serial(\n",
        "        # Use teacher forcing (feed output of previous step to current step)\n",
        "        tl.ShiftRight(mode=mode), # Specify the mode!\n",
        "        # Add positional encoder\n",
        "        positional_encoder,\n",
        "        # Add decoder blocks\n",
        "        decoder_blocks,\n",
        "        # Normalize layer\n",
        "        tl.LayerNorm(),\n",
        "\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
        "        tl.Dense(vocab_size),\n",
        "        # Get probabilities with Logsoftmax\n",
        "        tl.LogSoftmax(),\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q03ITGBKh7Da",
        "outputId": "b14e9c72-5652-4701-f3d4-eea5085bb80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        }
      },
      "source": [
        "print(TransformerLM(n_layers=1))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  ShiftRight(1)\n",
            "  Embedding_33300_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Branch_out3[\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "          ]\n",
            "          DotProductAttn_in3\n",
            "          AttnOutput\n",
            "          Dense_512\n",
            "        ]\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Relu\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nGNXEb7h9LC"
      },
      "source": [
        "def training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n",
        "    '''\n",
        "    Input:\n",
        "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
        "        train_gen (generator): Training stream of data.\n",
        "        eval_gen (generator): Evaluation stream of data.\n",
        "        output_dir (str): folder to save your file.\n",
        "        \n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop.\n",
        "    '''\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    train_task = training.TrainTask( \n",
        "      labeled_data=train_gen, # The training generator\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \n",
        "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
        "      lr_schedule=lr_schedule,\n",
        "      n_steps_per_checkpoint=10\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask( \n",
        "      labeled_data=eval_gen, # The evaluation generator\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    loop = training.Loop(TransformerLM(d_model=4,\n",
        "                                       d_ff=16,\n",
        "                                       n_layers=1,\n",
        "                                       n_heads=2,\n",
        "                                       mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    \n",
        "    return loop"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykfLZAk2iCKa",
        "outputId": "5da74485-d7ba-4e81-8a65-197fd1c6109f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "!rm -f ~/model/model.pkl.gz\n",
        "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
        "loop.run(10)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-6b85dd092d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -f ~/model/model.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-7179e69f07df>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(TransformerLM, train_gen, eval_gen, output_dir)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     train_task = training.TrainTask( \n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mlabeled_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The training generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mloss_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBK3_whWiKOd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}